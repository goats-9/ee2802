\documentclass[journal,12pt,twocolumn]{IEEEtran}
\usepackage{setspace}
\usepackage{gensymb}
\singlespacing
\usepackage[cmex10]{amsmath}
\usepackage{amsthm}
\usepackage{mathrsfs}
\usepackage{txfonts}
\usepackage{stfloats}
\usepackage{bm}
\usepackage{cite}
\usepackage{cases}
\usepackage{subfig}
\usepackage{longtable}
\usepackage{multirow}
\usepackage{enumitem}
\usepackage{mathtools}
\usepackage{tikz}
\usepackage{circuitikz}
\usepackage{verbatim}
\usepackage[breaklinks=true]{hyperref}
\usepackage{tkz-euclide} % loads  TikZ and tkz-base
\usepackage{listings}
\usepackage{color}    
\usepackage{array}    
\usepackage{longtable}
\usepackage{calc}     
\usepackage{multirow} 
\usepackage{hhline}   
\usepackage{ifthen}   
\usepackage{lscape}     
\usepackage{chngcntr}
\DeclareMathOperator*{\Res}{Res}
\renewcommand\thesection{\arabic{section}}
\renewcommand\thesubsection{\thesection.\arabic{subsection}}
\renewcommand\thesubsubsection{\thesubsection.\arabic{subsubsection}}

\renewcommand\thesectiondis{\arabic{section}}
\renewcommand\thesubsectiondis{\thesectiondis.\arabic{subsection}}
\renewcommand\thesubsubsectiondis{\thesubsectiondis.\arabic{subsubsection}}
\renewcommand\thetable{\arabic{table}}
% correct bad hyphenation here
\hyphenation{op-tical net-works semi-conduc-tor}
\def\inputGnumericTable{}                                 %%

\lstset{
%language=C,
frame=single, 
breaklines=true,
columns=fullflexible
}
%\lstset{
%language=tex,
%frame=single, 
%breaklines=true
%}

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\begin{document}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{problem}{Problem}
\newtheorem{proposition}{Proposition}[section]
\newtheorem{lemma}{Lemma}[section]
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{example}{Example}[section]
\newtheorem{definition}[problem]{Definition}
\newcommand{\BEQA}{\begin{eqnarray}}
\newcommand{\EEQA}{\end{eqnarray}}
\newcommand{\define}{\stackrel{\triangle}{=}}
\bibliographystyle{IEEEtran}
\providecommand{\mbf}{\mathbf}
\providecommand{\pr}[1]{\ensuremath{\Pr\left(#1\right)}}
\providecommand{\qfunc}[1]{\ensuremath{Q\left(#1\right)}}
\providecommand{\sbrak}[1]{\ensuremath{{}\left[#1\right]}}
\providecommand{\lsbrak}[1]{\ensuremath{{}\left[#1\right.}}
\providecommand{\rsbrak}[1]{\ensuremath{{}\left.#1\right]}}
\providecommand{\brak}[1]{\ensuremath{\left(#1\right)}}
\providecommand{\lbrak}[1]{\ensuremath{\left(#1\right.}}
\providecommand{\rbrak}[1]{\ensuremath{\left.#1\right)}}
\providecommand{\cbrak}[1]{\ensuremath{\left\{#1\right\}}}
\providecommand{\lcbrak}[1]{\ensuremath{\left\{#1\right.}}
\providecommand{\rcbrak}[1]{\ensuremath{\left.#1\right\}}}
\theoremstyle{remark}
\newtheorem{rem}{Remark}
\newcommand{\sgn}{\mathop{\mathrm{sgn}}}
\providecommand{\abs}[1]{\left\vert#1\right\vert}
\providecommand{\res}[1]{\Res\displaylimits_{#1}} 
\providecommand{\norm}[1]{\left\lVert#1\right\rVert}
\providecommand{\mtx}[1]{\mathbf{#1}}
\providecommand{\mean}[1]{E\left[ #1 \right]}   
\providecommand{\fourier}{\overset{\mathcal{F}}{ \rightleftharpoons}}
\providecommand{\system}[1]{\overset{\mathcal{#1}}{ \longleftrightarrow}}
\newcommand{\solution}{\noindent \textbf{Solution: }}
\newcommand{\cosec}{\,\text{cosec}\,}
\providecommand{\dec}[2]{\ensuremath{\overset{#1}{\underset{#2}{\gtrless}}}}
\newcommand{\myvec}[1]{\ensuremath{\begin{pmatrix}#1\end{pmatrix}}}
\newcommand{\mydet}[1]{\ensuremath{\begin{vmatrix}#1\end{vmatrix}}}
\renewcommand{\vec}[1]{\boldsymbol{\mathbf{#1}}}
\def\putbox#1#2#3{\makebox[0in][l]{\makebox[#1][l]{}\raisebox{\baselineskip}[0in][0in]{\raisebox{#2}[0in][0in]{#3}}}}
     \def\rightbox#1{\makebox[0in][r]{#1}}
     \def\centbox#1{\makebox[0in]{#1}}
     \def\topbox#1{\raisebox{-\baselineskip}[0in][0in]{#1}}
     \def\midbox#1{\raisebox{-0.5\baselineskip}[0in][0in]{#1}}

\vspace{3cm}
\title{Machine Learning for Grading Students}
\author{Gautam Singh}
\maketitle
\tableofcontents
\bigskip

\begin{abstract}
    This document is a report which compares the grade distribution obtained by
    using a method based on machine learning as compared to fitting a normal 
    curve to the scores of the students.
\end{abstract}

\section{Introduction}
We test the utility of the $K$-means algorithm in assigning grades
as compared to estimating the grades using the standard normal 
distribution.

We consider the scores of $N = 94$ students who have taken a course in the 
Indian Institute of Technology, Hyderabad (IITH) as our dataset.

\section{Fitting a Gaussian Curve}
Since $N$ is not very large, given the scores of each student 
$x_i,\ 1 \le i \le N$, we can compute the population mean and population 
variance as

\begin{align}
    \mu &= \mean{x} \\
    \sigma^2 &= \mean{\brak{x-\mu}^2}
    \label{eq:pop-param}
\end{align}

We assume that the scores $x \sim N\brak{\mu,\sigma^2}$. Thus, we compute the
$Z$-scores as
\begin{align}
    Z = \frac{x-\mu}{\sigma}
\end{align}

The grades are assigned as per Table \ref{tab:grade-scheme}.
\begin{table}[!ht]
    \centering
    \input{tables/grades.tex}
    \caption{Grading Scheme.}
    \label{tab:grade-scheme}
\end{table}

The Python code \texttt{codes/grades\_norm.py} takes the given input 
population dataset \texttt{marks.xlsx} and assigns grades appropriately. 
The grades are output to \texttt{grades.xlsx}.

\section{K-Means Clustering}
$K$-Means clustering is an unsupervised classification model, which attempts to 
cluster unlabeled data in order to gain more structure from it. 

We frame this requirement as an optimization problem. For a set of data points 
$\cbrak{\vec{x}_i}_{i=1}^N$ and means $\cbrak{\vec{\mu}_i}_{i=1}^K$, we define
for $1 \le n \le N,\ 1 \le k \le K$,
\begin{align}
    r_{nk} \triangleq
    \begin{cases}
        1 & \argmin_{j}\norm{\vec{x}_n-\vec{\mu}_j} = k \\
        0 & \textrm{otherwise}
    \end{cases}
    \label{eq:rnk-def}
\end{align}
Thus, we need to find points $\vec{\mu}_k$ minimizing the cost function
\begin{align}
    J \triangleq \sum_{n=1}^N\sum_{k=1}^Kr_{nk}\norm{\vec{x}_n-\vec{\mu}_k}^2
    \label{eq:cost}
\end{align}
Clearly, \eqref{eq:cost} is a quadratic function of $\vec{\mu}_k$.
Differentiating with respect to $\vec{\mu}_k$ and setting the derivative
to zero, we get
\begin{align}
    \sum_{n=1}^N2\vec{\mu}_kr_{nk}\brak{\vec{x}_n-\vec{\mu}_k} = 0 \\
    \implies \vec{{\mu_k}} = \frac{\sum_{n=1}^Nr_{nk}\vec{x}_n}{\sum_{n=1}^Nr_{nk}} = \frac{\vec{Xr}_k}{\vec{1}^\top\vec{r}_k}
    \label{eq:M-step}
\end{align}
where
\begin{align}
    \vec{X} &\triangleq \myvec{\vec{x}_1&\vec{x}_2&\ldots&\vec{x}_n} \\
    \vec{r}_k &\triangleq \myvec{r_{1k}&r_{2k}&\ldots&r_{nk}}^\top \\
    \vec{1} &\triangleq \myvec{1&1&\ldots&1}^\top
    \label{eq:M-defs}
\end{align}
From \eqref{eq:M-step}, we see that the optimum is attained when $\vec{\mu}_k$ 
is set to the expectation of the $\vec{x}_n$ with respect to $r_{nk}$.

Thus, the $K$-means algorithm is essentially an \textit{EM algorithm}, where 
each iteration consists of two steps.
\begin{enumerate}
    \item \textit{E Step}: Calculate the $K$-expected values
    \begin{align}
        \tilde{\vec{\mu}_k} \triangleq \frac{\sum_{n=1}^Nr_{nk}\vec{x}_n}{\sum_{n=1}^Nr_{nk}}
        \label{eq:E-step}
    \end{align}
    for $1 \le k \le K$.
    \item \textit{M Step}: Assign $\vec{\mu}_k \leftarrow \tilde{\vec{\mu}_k}$
    for $1 \le k \le K$.
\end{enumerate}

\section{Results}
The grade distribution using each method is shown in Fig. \ref{fig:gauss-dist}
and Fig. \ref{fig:kmeans-dist}.
\begin{figure}[!ht]
    \centering
    \includegraphics[width=\columnwidth]{figs/grades_gauss.png}
    \caption{Grade distribution using a Gaussian curve.}
    \label{fig:gauss-dist}
\end{figure}
\begin{figure}[!ht]
    \centering
    \includegraphics[width=\columnwidth]{figs/grades_kmeans.png}
    \caption{Grade distribution using the $K$-means algorithm.}
    \label{fig:kmeans-dist}
\end{figure}
Based on the results, we can make the following observations:
\begin{enumerate}
    \item Grading using the Gaussian distribution would lead to many students
    failing the course, while this is not the case using the $K$-means
    algorithm.
    \item Using the Gaussian distribution is quite unfair, since there could
    be students with quite similar marks but with a difference in grade, just
    because they lie on either side of a predefined boundary.
    \item The $K$-means algorithm allows for better decision boundaries,
    depending on how skewed the performance of the students is, accordingly
    to the difficulty of the course.
    \item Unlike the Gaussian distribution, the $K$-means algorithm can be
    used for a fairer assignment of the grades, no matter how skewed the 
    performance of students in a course is.
\end{enumerate}
\end{document}
