\documentclass[journal,12pt,twocolumn]{IEEEtran}
\usepackage{setspace}
\usepackage{gensymb}
\usepackage{xcolor}
\usepackage{caption}
\singlespacing
\usepackage{siunitx}
\usepackage[cmex10]{amsmath}
\usepackage{mathtools}
\usepackage{hyperref}
\usepackage{amsthm}
\usepackage{mathrsfs}
\usepackage{txfonts}
\usepackage{stfloats}
\usepackage{cite}
\usepackage{cases}
\usepackage{subfig}
\usepackage{longtable}
\usepackage{multirow}
\usepackage{enumitem}
\usepackage{bm}
\usepackage{mathtools}
\usepackage{listings}
\usepackage{tikz}
\usetikzlibrary{shapes,arrows,positioning}
\usepackage{circuitikz}
\renewcommand{\vec}[1]{\boldsymbol{\mathbf{#1}}}
\DeclareMathOperator*{\Res}{Res}
\renewcommand\thesection{\arabic{section}}
\renewcommand\thesubsection{\thesection.\arabic{subsection}}
\renewcommand\thesubsubsection{\thesubsection.\arabic{subsubsection}}

\renewcommand\thesectiondis{\arabic{section}}
\renewcommand\thesubsectiondis{\thesectiondis.\arabic{subsection}}
\renewcommand\thesubsubsectiondis{\thesubsectiondis.\arabic{subsubsection}}
\hyphenation{op-tical net-works semi-conduc-tor}

\lstset{
language=Python,
frame=single, 
breaklines=true,
columns=fullflexible
}
\begin{document}
\theoremstyle{definition}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{problem}{Problem}
\newtheorem{proposition}{Proposition}[section]
\newtheorem{lemma}{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{example}{Example}[section]
\newtheorem{definition}{Definition}[section]
\newcommand{\BEQA}{\begin{eqnarray}}
\newcommand{\EEQA}{\end{eqnarray}}
\newcommand{\define}{\stackrel{\triangle}{=}}
\newcommand{\myvec}[1]{\ensuremath{\begin{pmatrix}#1\end{pmatrix}}}
\newcommand{\mydet}[1]{\ensuremath{\begin{vmatrix}#1\end{vmatrix}}}
\bibliographystyle{IEEEtran}
\providecommand{\nCr}[2]{\,^{#1}C_{#2}} % nCr
\providecommand{\nPr}[2]{\,^{#1}P_{#2}} % nPr
\providecommand{\mbf}{\mathbf}
\providecommand{\pr}[1]{\ensuremath{\Pr\left(#1\right)}}
\providecommand{\qfunc}[1]{\ensuremath{Q\left(#1\right)}}
\providecommand{\sbrak}[1]{\ensuremath{{}\left[#1\right]}}
\providecommand{\lsbrak}[1]{\ensuremath{{}\left[#1\right.}}
\providecommand{\rsbrak}[1]{\ensuremath{{}\left.#1\right]}}
\providecommand{\brak}[1]{\ensuremath{\left(#1\right)}}
\providecommand{\lbrak}[1]{\ensuremath{\left(#1\right.}}
\providecommand{\rbrak}[1]{\ensuremath{\left.#1\right)}}
\providecommand{\cbrak}[1]{\ensuremath{\left\{#1\right\}}}
\providecommand{\lcbrak}[1]{\ensuremath{\left\{#1\right.}}
\providecommand{\rcbrak}[1]{\ensuremath{\left.#1\right\}}}
\theoremstyle{remark}
\newtheorem{rem}{Remark}
\newcommand{\sgn}{\mathop{\mathrm{sgn}}}
\newcommand{\rect}{\mathop{\mathrm{rect}}}
\newcommand{\sinc}{\mathop{\mathrm{sinc}}}
\providecommand{\abs}[1]{\left\vert#1\right\vert}
\providecommand{\res}[1]{\Res\displaylimits_{#1}} 
\providecommand{\norm}[1]{\left\Vert#1\right\Vert}
\providecommand{\mtx}[1]{\mathbf{#1}}
\providecommand{\mean}[1]{E\left[ #1 \right]}
\providecommand{\fourier}{\overset{\mathcal{F}}{ \rightleftharpoons}}
\providecommand{\ztrans}{\overset{\mathcal{Z}}{ \rightleftharpoons}}
\providecommand{\system}[1]{\overset{\mathcal{#1}}{ \longleftrightarrow}}
\newcommand{\solution}{\noindent \textbf{Solution: }}
\providecommand{\dec}[2]{\ensuremath{\overset{#1}{\underset{#2}{\gtrless}}}}
\let\StandardTheFigure\thefigure
\def\putbox#1#2#3{\makebox[0in][l]{\makebox[#1][l]{}\raisebox{\baselineskip}[0in][0in]{\raisebox{#2}[0in][0in]{#3}}}}
     \def\rightbox#1{\makebox[0in][r]{#1}}
     \def\centbox#1{\makebox[0in]{#1}}
     \def\topbox#1{\raisebox{-\baselineskip}[0in][0in]{#1}}
     \def\midbox#1{\raisebox{-0.5\baselineskip}[0in][0in]{#1}}

\vspace{3cm}
\title{Line Assignment}
\author{Gautam Singh}
\maketitle
\bigskip

\begin{abstract}
    This document contains a general solution to Question 16 of 
    Exercise 2 in Chapter 11 of the class 12 NCERT textbook.
\end{abstract}

\begin{enumerate}
    \item Find the shortest distance between the lines whose vector equations are
    \begin{align}
        L_1: \vec{x} = \vec{x_1} + \lambda_1\vec{m_1} \label{eq:L1} \\
        L_2: \vec{x} = \vec{x_2} + \lambda_2\vec{m_2} \label{eq:L2}
    \end{align}

    \solution Let $\vec{A}$ and $\vec{B}$ be points on lines $L_1$ and $L_2$
    respectively such that $AB$ is normal to both lines. Define
    \begin{align}
        \vec{M} &\triangleq \myvec{\vec{m_1} & \vec{m_2}} \label{eq:M-def} \\
        \vec{\lambda} &\triangleq \myvec{\lambda_1\\-\lambda_2} \label{eq:lambda-def} \\
        \vec{x} &\triangleq \vec{x_2} - \vec{x_1} \label{eq:x-def}
    \end{align}
    Then, we have the following equations:
    \begin{align}
        \vec{A} = \vec{x_1} + \lambda_1\vec{m_1} \label{eq:A-def} \\
        \vec{B} = \vec{x_2} + \lambda_2\vec{m_2} \label{eq:B-def}
    \end{align}
    From \eqref{eq:A-def} and \eqref{eq:B-def}, define the real-valued function
    $f$ as
    \begin{align}
        f\brak{\vec{\lambda}} &\triangleq \norm{\vec{A}-\vec{B}}^2 \\
                              &= \norm{\vec{M}\vec{\lambda}-\vec{x}}^2 \\
                              &= \brak{\vec{M\lambda}-\vec{x}}^\top\brak{\vec{M\lambda}-\vec{x}} \\
                              &= \vec{\lambda}^\top\brak{\vec{M}^\top\vec{M}}\vec{\lambda} - 2\vec{x}^\top\vec{M\lambda} + \norm{\vec{x}}^2
        \label{eq:f-def}
    \end{align}
    From \eqref{eq:f-def}, we see that $f$ is quadratic in $\vec{\lambda}$.

    We now prove a useful lemma here.
    \begin{lemma}
        The quadratic form
        \begin{align}
            q\brak{\vec{x}} \triangleq \vec{x}^\top\vec{Ax} + \vec{b}^\top\vec{x} + c
            \label{eq:quad-x}
        \end{align}
        is convex iff $\vec{A}$ is positive semi-definite.
    \end{lemma}
    \begin{proof}
        Consider two points $\vec{x_1}$ and $\vec{x_2}$, and a real constant
        $0 \le \mu \le 1$. Then,
        \begin{align}
            &\mu f\brak{\vec{x_1}} + \brak{1-\mu}f\brak{\vec{x_2}} - f\brak{\mu\vec{x_1}+\brak{1-\mu}\vec{x_2}} \nonumber \\
            &= \brak{\mu-\mu^2}\vec{x_1}^\top\vec{Ax_1} + \brak{1-\mu-\brak{1-\mu}^2}\vec{x_2}^\top\vec{Ax_2} \nonumber \\
            &- 2\mu\brak{1-\mu}\vec{x_1}^\top\vec{Ax_2} \\
            &= \mu\brak{1-\mu}\brak{\vec{x_1}^\top\vec{Ax_1}-2\vec{x_1}^\top\vec{Ax_2}+\vec{x_2}^\top\vec{Ax_2}} \\
            &= \mu\brak{1-\mu}\brak{\vec{x_1}-\vec{x_2}}^\top\vec{A}\brak{\vec{x_1}-\vec{x_2}}
            \label{eq:psd-iff}
        \end{align}
        Since $\vec{x_1}$ and $\vec{x_2}$ are arbitrary, it follows from 
        \eqref{eq:psd-iff} that
        \begin{align}
            \mu f\brak{\vec{x_1}} + \brak{1-\mu}f\brak{\vec{x_2}} \ge f\brak{\mu\vec{x_1}+\brak{1-\mu}\vec{x_2}}
        \end{align}
        iff $\vec{A}$ is positive semi-definite, as required.
    \end{proof}
    Using the above lemma, we show that $f$ is convex by showing that 
    $\vec{M}^\top\vec{M}$ is positive semi-definite. Indeed, for any 
    $\vec{p} \triangleq \myvec{x\\y}$,
    \begin{align}
        \vec{p}^\top\vec{M}^\top\vec{Mp} = \norm{\vec{Mp}}^2 \ge 0
        \label{eq:psd}
    \end{align}
    and thus, $f$ is convex.

    We need to minimize $f$ as a function of $\vec{\lambda}$. Thus, 
    differentiating \eqref{eq:f-def} using the chain rule,
    \begin{align}
        \frac{df\brak{\vec{\lambda}}}{d\vec{\lambda}} &= \vec{M}^\top\brak{\vec{M\lambda}-\vec{x}}+\vec{M}\brak{\vec{M\lambda}-\vec{x}}^\top \\
                                                      &= 2\vec{M}^\top\brak{\vec{M\lambda}-\vec{x}}
        \label{eq:vec-min}
    \end{align}
    Setting \eqref{eq:vec-min} to zero gives
    \begin{align}
        \vec{M}^\top\vec{M\lambda} = \vec{M}^\top\vec{x}
        \label{eq:vec-eqn}
    \end{align}
    We have the following cases:
    \begin{enumerate}
        \item There exists a $\vec{\lambda}$ satisfying
        \begin{align}
            \vec{M}\vec{\lambda} &= \vec{x} \\
            \implies \lambda_1\vec{m_1} - \lambda_2\vec{m_2} &= \vec{x_2}-\vec{x_1} \\
            \implies \vec{x_1} + \lambda_1\vec{m_1} &= \vec{x_2} + \lambda_2\vec{m_2}
            \label{eq:intersect}
        \end{align}
        Thus, both lines intersect at a point and the shortest
        distance between them is 0. To check for the existence of such a 
        $\vec{\lambda}$, we can bring the augmented matrix 
        $\myvec{\vec{M}&\vec{x}}$ into row-reduced echelon form and check 
        whether there is a pivot in the last column.

        \item $\vec{M}^\top\vec{M}$ is singular. Since $\vec{M}^\top\vec{M}$ is a 
        sqaure matrix of order 2, its rank must be 1. Further,
        \begin{align}
            \det\brak{\vec{M}^\top\vec{M}} &= \mydet{\vec{m_1}^\top\vec{m_1} & \vec{m_1}^\top\vec{m_2} \\
                                                \vec{m_1}^\top\vec{m_2} & \vec{m_2}^\top\vec{m_2}} \\
                                      &= \brak{\norm{\vec{m_1}}.\norm{\vec{m_2}}}^2 - \brak{\vec{m_1}^\top\vec{m_2}}^2
                                      \label{eq:det-parallel}
        \end{align}
        Thus, equating the determinant to zero gives
        \begin{align}
            \norm{\vec{m_1}}.\norm{\vec{m_2}} = \abs{\vec{m_1}^\top\vec{m_2}}
            \label{eq:vec-parallel}
        \end{align}
        which implies that both lines are parallel to each other. Setting 
        $\vec{m_2} = k\vec{m_1}, k\in\mathbb{R}\setminus\cbrak{0}$, we obtain one
        equation from \eqref{eq:vec-eqn}.
        \begin{align}
            \vec{m_1}^\top\vec{m_1}\brak{\lambda_1-k\lambda_2} = \vec{m_1}^\top\vec{x} \\
            \implies \lambda_1-k\lambda_2 = \frac{\vec{m_1}^\top\vec{x}}{\norm{\vec{m_1}}^2}
            \label{eq:lambda-expr}
        \end{align}
        Therefore, the required shortest distance is
        \begin{align}
            \norm{\vec{A}-\vec{B}} = \norm{\frac{\vec{m_1}^\top\vec{x}\vec{m_1}}{\norm{\vec{m_1}}^2}-\vec{x}}
            \label{eq:dist-parallel}
        \end{align}

        \item $\vec{M}^\top\vec{M}$ is nonsinglar. This implies that the lines
        are skew. From \eqref{eq:vec-eqn},
        \begin{align}
            \vec{\lambda} = \brak{\vec{M}^\top\vec{M}}^{-1}\vec{M}^\top\vec{x}
            \label{eq:lambda-skew}
        \end{align}
        and therefore, the shortest distance is
        \begin{align}
            \norm{\vec{A}-\vec{B}} &= \norm{\brak{\vec{M}\brak{\vec{M}^\top\vec{M}}^{-1}\vec{M}^\top - \vec{I_n}}\vec{x}} 
            \label{eq:dist-skew}
        \end{align}
        where $\vec{I_n}$ is the identity matrix of order $n$.
    \end{enumerate}
\end{enumerate}
\end{document}
